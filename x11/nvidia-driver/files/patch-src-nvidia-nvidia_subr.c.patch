--- src/nvidia/nvidia_subr.c.orig	2020-02-21 03:55:08.000000000 +0300
+++ src/nvidia/nvidia_subr.c	2020-03-21 06:32:11.793431000 +0300
@@ -1822,25 +1822,254 @@
     return NV_FALSE;
 }
 
+#include <asm/atomic.h>      // atomic_t
+#undef BIT
+#include <linux/workqueue.h> // work_struct
+#include <linux/semaphore.h> // semaphore
+#include <linux/mm.h>        // get_order
+
+#define	page_to_phys(x) VM_PAGE_TO_PHYS(x) // source: sys/dev/drm2/drm_os_freebsd.h
+
+// source: common/inc/nv-linux.h
+
+typedef struct nv_alloc_s {
+    struct nv_alloc_s *next;
+    struct device     *dev;
+    atomic_t       usage_count;
+    struct {
+        NvBool contig   : 1;
+        NvBool guest    : 1;
+        NvBool zeroed   : 1;
+        NvBool aliased  : 1;
+        NvBool user     : 1;
+        NvBool node0    : 1;
+        NvBool peer_io  : 1;
+        NvBool physical : 1;
+    } flags;
+    unsigned int   cache_type;
+    unsigned int   num_pages;
+    unsigned int   order;
+    unsigned int   size;
+    nvidia_pte_t **page_table;          /* list of physical pages allocated */
+    unsigned int   pid;
+    struct page  **user_pages;
+    NvU64         guest_id;             /* id of guest VM */
+    void          *import_priv;
+} nv_alloc_t;
+
+#define NV_DBG_MEMINFO NV_DBG_INFO
+
+#if !defined(DEBUG) && defined(__GFP_NOWARN)
+#define NV_GFP_KERNEL (GFP_KERNEL | __GFP_NOWARN)
+#define NV_GFP_ATOMIC (GFP_ATOMIC | __GFP_NOWARN)
+#else
+#define NV_GFP_KERNEL (GFP_KERNEL)
+#define NV_GFP_ATOMIC (GFP_ATOMIC)
+#endif
+
+#define NV_MEMDBG_ADD(ptr, size)    // source: common/inc/nv-memdbg.h
+#define NV_MEMDBG_REMOVE(ptr, size) // source: common/inc/nv-memdbg.h
+
+#define NV_KMALLOC(ptr, size) \
+    { \
+        (ptr) = kmalloc(size, NV_GFP_KERNEL); \
+        if (ptr) \
+            NV_MEMDBG_ADD(ptr, size); \
+    }
+
+#define NV_KFREE(ptr, size) \
+    { \
+        NV_MEMDBG_REMOVE(ptr, size); \
+        kfree((void *) (ptr)); \
+    }
+
+#define NV_ATOMIC_READ(data)            atomic_read(&(data))
+#define NV_ATOMIC_SET(data,val)         atomic_set(&(data), (val))
+
+#define NV_KMEM_CACHE_ALLOC(kmem_cache)     \
+    kmem_cache_alloc(kmem_cache, GFP_KERNEL)
+#define NV_KMEM_CACHE_FREE(ptr, kmem_cache) \
+    kmem_cache_free(kmem_cache, ptr)
+
+// !common/inc/nv-linux.h
+
+// source: nvidia/nv.c
+
+static
+int nvos_free_alloc(
+    nv_alloc_t *at
+)
+{
+    unsigned int i;
+
+    if (at == NULL)
+        return -1;
+
+    if (NV_ATOMIC_READ(at->usage_count))
+        return 1;
+
+    for (i = 0; i < at->num_pages; i++)
+    {
+        if (at->page_table[i] != NULL)
+            kfree(at->page_table[i]); //NV_KMEM_CACHE_FREE(at->page_table[i], nvidia_pte_t_cache);
+    }
+    os_free_mem(at->page_table);
+
+    NV_KFREE(at, sizeof(nv_alloc_t));
+
+    return 0;
+}
+
+static
+nv_alloc_t *nvos_create_alloc(
+    struct device *dev,
+    int num_pages
+)
+{
+    nv_alloc_t *at;
+    unsigned int pt_size, i;
+
+    NV_KMALLOC(at, sizeof(nv_alloc_t));
+    if (at == NULL)
+    {
+        nv_printf(NV_DBG_ERRORS, "NVRM: failed to allocate alloc info\n");
+        return NULL;
+    }
+
+    memset(at, 0, sizeof(nv_alloc_t));
+
+    at->dev = dev;
+    pt_size = num_pages * sizeof(nvidia_pte_t *);
+    if (os_alloc_mem((void **)&at->page_table, pt_size) != NV_OK)
+    {
+        nv_printf(NV_DBG_ERRORS, "NVRM: failed to allocate page table\n");
+        NV_KFREE(at, sizeof(nv_alloc_t));
+        return NULL;
+    }
+
+    memset(at->page_table, 0, pt_size);
+    at->num_pages = num_pages;
+    NV_ATOMIC_SET(at->usage_count, 0);
+
+    for (i = 0; i < at->num_pages; i++)
+    {
+        at->page_table[i] = kmalloc(sizeof(nvidia_pte_t), NV_GFP_KERNEL); //NV_KMEM_CACHE_ALLOC(nvidia_pte_t_cache);
+        if (at->page_table[i] == NULL)
+        {
+            nv_printf(NV_DBG_ERRORS,
+                      "NVRM: failed to allocate page table entry\n");
+            nvos_free_alloc(at);
+            return NULL;
+        }
+        memset(at->page_table[i], 0, sizeof(nvidia_pte_t));
+    }
+
+    at->pid = os_get_current_process();
+
+    return at;
+}
+
+/*
+ * By registering user pages, we create a dummy nv_alloc_t for it, so that the
+ * rest of the RM can treat it like any other alloc.
+ *
+ * This also converts the page array to an array of physical addresses.
+ */
 NV_STATUS NV_API_CALL nv_register_user_pages(
     nv_state_t *nv,
     NvU64       page_count,
     NvU64      *phys_addr,
     void       *import_priv,
-    void      **priv
+    void      **priv_data
 )
 {
+    nv_alloc_t *at;
+    NvU64 i;
+    struct page **user_pages;
+    //nv_linux_state_t *nvl;
+    nvidia_pte_t *page_ptr;
+
+    nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_register_user_pages: 0x%x\n", page_count);
+    user_pages = *priv_data;
+    //nvl = NV_GET_NVL_FROM_NV_STATE(nv);
+
+    at = nvos_create_alloc(/*nvl->dev*/ NULL, page_count);
+
+    if (at == NULL)
+    {
+        nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_register_user_pages: Oh crap\n");
+        return NV_ERR_NO_MEMORY;
+    }
+
+    /*
+     * Anonymous memory currently must be write-back cacheable, and we can't
+     * enforce contiguity.
+     */
+    at->cache_type = NV_MEMORY_UNCACHED;
+//~ #if defined(NVCPU_FAMILY_ARM)
+    //~ at->flags.aliased = NV_TRUE;
+//~ #endif
+
+    at->flags.user = NV_TRUE;
+
+    at->order = get_order(at->num_pages * PAGE_SIZE);
+
+    for (i = 0; i < page_count; i++)
+    {
+        /*
+         * We only assign the physical address and not the DMA address, since
+         * this allocation hasn't been DMA-mapped yet.
+         */
+        page_ptr = at->page_table[i];
+        page_ptr->physical_address = page_to_phys(user_pages[i]);
+
+        phys_addr[i] = page_ptr->physical_address;
+    }
+
+    /* Save off the user pages array to be restored later */
+    at->user_pages = user_pages;
+
+    /* Save off the import private data to be returned later */
+    if (import_priv != NULL)
+    {
+        at->import_priv = import_priv;
+    }
+
+    *priv_data = at;
+
+    //~ NV_PRINT_AT(NV_DBG_MEMINFO, at);
+
     return NV_OK;
 }
 
 void NV_API_CALL nv_unregister_user_pages(
     nv_state_t *nv,
     NvU64       page_count,
     void      **import_priv,
-    void      **priv
+    void      **priv_data
 )
 {
+    nv_alloc_t *at = *priv_data;
+
+    nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_unregister_user_pages: 0x%x\n", page_count);
+
+    //~ NV_PRINT_AT(NV_DBG_MEMINFO, at);
+
+    WARN_ON(!at->flags.user);
+
+    /* Restore the user pages array for the caller to handle */
+    *priv_data = at->user_pages;
+
+    /* Return the import private data for the caller to handle */
+    if (import_priv != NULL)
+    {
+        *import_priv = at->import_priv;
+    }
+
+    nvos_free_alloc(at);
 }
+
+// !nvidia/nv.c
 
 NV_STATUS NV_API_CALL nv_get_device_memory_config(
     nv_state_t *nv,
