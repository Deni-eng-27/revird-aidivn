--- src/nvidia/nvidia_subr.c.orig	2020-02-21 03:55:08.000000000 +0300
+++ src/nvidia/nvidia_subr.c	2020-03-17 23:42:44.118010000 +0300
@@ -1729,24 +1729,282 @@
     return NV_FALSE;
 }
 
+#include <asm/atomic.h>      // atomic_t
+#include <linux/workqueue.h> // work_struct
+#include <linux/semaphore.h> // semaphore
+#include <linux/mm.h>        // get_order
+
+#define	page_to_phys(x) VM_PAGE_TO_PHYS(x) // source: sys/dev/drm2/drm_os_freebsd.h
+
+// source: common/inc/nv-linux.h
+
+#define NV_MAX_REGISTRY_KEYS_LENGTH   512
+
+typedef struct nv_work_s {
+    struct work_struct task;
+    void *data;
+} nv_work_t;
+
+/* linux-specific version of old nv_state_t */
+/* this is a general os-specific state structure. the first element *must* be
+   the general state structure, for the generic unix-based code */
+typedef struct nv_linux_state_s {
+    nv_state_t nv_state;
+
+    atomic_t usage_count;
+    NvU32    suspend_count;
+
+    struct device  *dev;
+    struct pci_dev *pci_dev;
+
+    // ...
+
+} nv_linux_state_t;
+
+#define NV_GET_NVL_FROM_NV_STATE(nv)    ((nv_linux_state_t *)nv->os_state)
+
+typedef struct nv_alloc_s {
+    struct nv_alloc_s *next;
+    struct device     *dev;
+    atomic_t       usage_count;
+    unsigned int   flags;
+    unsigned int   num_pages;
+    unsigned int   order;
+    unsigned int   size;
+    nvidia_pte_t **page_table;          /* list of physical pages allocated */
+    unsigned int   pid;
+    struct page  **user_pages;
+    NvU64         guest_id;             /* id of guest VM */
+} nv_alloc_t;
+
+#define NV_DBG_MEMINFO NV_DBG_INFO
+
+#if !defined(DEBUG) && defined(__GFP_NOWARN)
+#define NV_GFP_KERNEL (GFP_KERNEL | __GFP_NOWARN)
+#define NV_GFP_ATOMIC (GFP_ATOMIC | __GFP_NOWARN)
+#else
+#define NV_GFP_KERNEL (GFP_KERNEL)
+#define NV_GFP_ATOMIC (GFP_ATOMIC)
+#endif
+
+#define NV_MEMDBG_ADD(ptr, size)    // source: common/inc/nv-memdbg.h
+#define NV_MEMDBG_REMOVE(ptr, size) // source: common/inc/nv-memdbg.h
+
+#define NV_KMALLOC(ptr, size) \
+    { \
+        (ptr) = kmalloc(size, NV_GFP_KERNEL); \
+        if (ptr) \
+            NV_MEMDBG_ADD(ptr, size); \
+    }
+
+#define NV_KFREE(ptr, size) \
+    { \
+        NV_MEMDBG_REMOVE(ptr, size); \
+        kfree((void *) (ptr)); \
+    }
+
+#define NV_ATOMIC_READ(data)            atomic_read(&(data))
+#define NV_ATOMIC_SET(data,val)         atomic_set(&(data), (val))
+
+#define NV_KMEM_CACHE_ALLOC(kmem_cache)     \
+    kmem_cache_alloc(kmem_cache, GFP_KERNEL)
+#define NV_KMEM_CACHE_FREE(ptr, kmem_cache) \
+    kmem_cache_free(kmem_cache, ptr)
+
+#define NV_ALLOC_TYPE_PCI               (1<<0)
+#define NV_ALLOC_TYPE_CONTIG            (1<<2)
+#define NV_ALLOC_TYPE_GUEST             (1<<3)
+#define NV_ALLOC_TYPE_ZEROED            (1<<4)
+#define NV_ALLOC_TYPE_ALIASED           (1<<5)
+#define NV_ALLOC_TYPE_USER              (1<<6)
+#define NV_ALLOC_TYPE_NODE0             (1<<7)
+#define NV_ALLOC_TYPE_PEER_IO           (1<<8)
+#define NV_ALLOC_TYPE_PHYSICAL          (1<<9)
+
+#define NV_ALLOC_MAPPING_SHIFT      16
+#define NV_ALLOC_ENC_MAPPING(flags) ((flags)<<NV_ALLOC_MAPPING_SHIFT)
+
+static inline NvU32 nv_alloc_init_flags(int cached, int contiguous, int zeroed)
+{
+    NvU32 flags = NV_ALLOC_ENC_MAPPING(cached);
+    flags |= NV_ALLOC_TYPE_PCI;
+    if (contiguous)
+        flags |= NV_ALLOC_TYPE_CONTIG;
+    if (zeroed)
+        flags |= NV_ALLOC_TYPE_ZEROED;
+#if defined(NVCPU_FAMILY_ARM)
+    if (!NV_ALLOC_MAPPING_CACHED(flags))
+        flags |= NV_ALLOC_TYPE_ALIASED;
+#endif
+    return flags;
+}
+
+#define NV_ALLOC_MAPPING_USER(flags)              ((flags) & NV_ALLOC_TYPE_USER)
+
+// !common/inc/nv-linux.h
+
+// source: nvidia/nv.c
+
+static
+int nvos_free_alloc(
+    nv_alloc_t *at
+)
+{
+    unsigned int i;
+
+    if (at == NULL)
+        return -1;
+
+    if (NV_ATOMIC_READ(at->usage_count))
+        return 1;
+
+    for (i = 0; i < at->num_pages; i++)
+    {
+        if (at->page_table[i] != NULL)
+            kfree(at->page_table[i]); //NV_KMEM_CACHE_FREE(at->page_table[i], nvidia_pte_t_cache);
+    }
+    os_free_mem(at->page_table);
+
+    NV_KFREE(at, sizeof(nv_alloc_t));
+
+    return 0;
+}
+
+static
+nv_alloc_t *nvos_create_alloc(
+    struct device *dev,
+    int num_pages
+)
+{
+    nv_alloc_t *at;
+    unsigned int pt_size, i;
+
+    NV_KMALLOC(at, sizeof(nv_alloc_t));
+    if (at == NULL)
+    {
+        nv_printf(NV_DBG_ERRORS, "NVRM: failed to allocate alloc info\n");
+        return NULL;
+    }
+
+    memset(at, 0, sizeof(nv_alloc_t));
+
+    at->dev = dev;
+    pt_size = num_pages * sizeof(nvidia_pte_t *);
+    if (os_alloc_mem((void **)&at->page_table, pt_size) != NV_OK)
+    {
+        nv_printf(NV_DBG_ERRORS, "NVRM: failed to allocate page table\n");
+        NV_KFREE(at, sizeof(nv_alloc_t));
+        return NULL;
+    }
+
+    memset(at->page_table, 0, pt_size);
+    at->num_pages = num_pages;
+    NV_ATOMIC_SET(at->usage_count, 0);
+
+    for (i = 0; i < at->num_pages; i++)
+    {
+        at->page_table[i] = kmalloc(sizeof(nvidia_pte_t), NV_GFP_KERNEL); //NV_KMEM_CACHE_ALLOC(nvidia_pte_t_cache);
+        if (at->page_table[i] == NULL)
+        {
+            nv_printf(NV_DBG_ERRORS,
+                      "NVRM: failed to allocate page table entry\n");
+            nvos_free_alloc(at);
+            return NULL;
+        }
+        memset(at->page_table[i], 0, sizeof(nvidia_pte_t));
+    }
+
+    at->pid = os_get_current_process();
+
+    return at;
+}
+
+/*
+ * By registering user pages, we create a dummy nv_alloc_t for it, so that the
+ * rest of the RM can treat it like any other alloc.
+ *
+ * This also converts the page array to an array of physical addresses.
+ */
 NV_STATUS NV_API_CALL nv_register_user_pages(
     nv_state_t *nv,
     NvU64       page_count,
     NvU64      *phys_addr,
-    void      **priv
+    void      **priv_data
 )
 {
+    nv_alloc_t *at;
+    NvU64 i;
+    struct page **user_pages;
+    //nv_linux_state_t *nvl;
+    struct nvidia_softc *nvl;
+    nvidia_pte_t *page_ptr;
+
+    nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_register_user_pages: 0x%x\n", page_count);
+    user_pages = *priv_data;
+    //nvl = NV_GET_NVL_FROM_NV_STATE(nv); // TODO: might be (incompatible) FreeBSD state
+    nvl = nv->os_state;
+
+    at = nvos_create_alloc(nvl->dev, page_count);
+
+    if (at == NULL)
+    {
+        nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_register_user_pages: Oh crap\n");
+        return NV_ERR_NO_MEMORY;
+    }
+
+    /*
+     * Anonymous memory currently must be write-back cacheable, and we can't
+     * enforce contiguity.
+     */
+    at->flags = nv_alloc_init_flags(1, 0, 0);
+    at->flags |= NV_ALLOC_TYPE_USER;
+
+    at->order = get_order(at->num_pages * PAGE_SIZE);
+
+    for (i = 0; i < page_count; i++)
+    {
+        /*
+         * We only assign the physical address and not the DMA address, since
+         * this allocation hasn't been DMA-mapped yet.
+         */
+        page_ptr = at->page_table[i];
+        page_ptr->physical_address = page_to_phys(user_pages[i]);
+
+        phys_addr[i] = page_ptr->physical_address;
+    }
+
+    /* Save off the user pages array to be restored later */
+    at->user_pages = user_pages;
+    *priv_data = at;
+
+    //~ NV_PRINT_AT(NV_DBG_MEMINFO, at);
+
     return NV_OK;
 }
 
 NV_STATUS NV_API_CALL nv_unregister_user_pages(
     nv_state_t *nv,
     NvU64       page_count,
-    void      **priv
+    void      **priv_data
 )
 {
+    nv_alloc_t *at = *priv_data;
+
+    nv_printf(NV_DBG_MEMINFO, "NVRM: VM: nv_unregister_user_pages: 0x%x\n", page_count);
+
+    //~ NV_PRINT_AT(NV_DBG_MEMINFO, at);
+
+    WARN_ON(!NV_ALLOC_MAPPING_USER(at->flags));
+
+    /* Restore the user pages array for the caller to handle */
+    *priv_data = at->user_pages;
+
+    nvos_free_alloc(at);
+
     return NV_OK;
 }
+
+// !nvidia/nv.c
 
 NV_STATUS NV_API_CALL nv_get_device_memory_config(
     nv_state_t *nv,
